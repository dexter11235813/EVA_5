{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.mnist_model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(8, 12, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(12, 16, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(16, 32, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(32, 24, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(24),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(24, 20, 3, bias=False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(20),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(20,16,3,bias = False, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(16,16,3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 10, 3),\n",
    "            nn.AvgPool2d(3)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.mnist_model(x)\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "batch_size = 128 \n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "        transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "    ])), batch_size=batch_size, shuffle=True,**kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        \n",
    "        transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "    ])), batch_size=batch_size, shuffle=True,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    train_correct = 0\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for _, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        train_correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss = F.nll_loss(output, target)\n",
    "        train_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for param in optimizer.param_groups:\n",
    "          lr = param['lr']\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    \n",
    "    #return 100.0 * train_correct/len(train_loader.dataset)\n",
    "    print (f'\\nTrain set: Average loss: {train_loss}, Train Accuracy: {train_correct}/{len(train_loader.dataset)} ({100. * train_correct / len(train_loader.dataset)}), learning rate : {lr}')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "   # return  1.0 * correct / len(test_loader.dataset)\n",
    "\n",
    "    print (f'\\nTest set: Average loss: {test_loss}, Test Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 8, 28, 28]              72\n              ReLU-2            [-1, 8, 28, 28]               0\n       BatchNorm2d-3            [-1, 8, 28, 28]              16\n           Dropout-4            [-1, 8, 28, 28]               0\n            Conv2d-5           [-1, 12, 28, 28]             864\n              ReLU-6           [-1, 12, 28, 28]               0\n       BatchNorm2d-7           [-1, 12, 28, 28]              24\n           Dropout-8           [-1, 12, 28, 28]               0\n            Conv2d-9           [-1, 16, 28, 28]           1,728\n             ReLU-10           [-1, 16, 28, 28]               0\n      BatchNorm2d-11           [-1, 16, 28, 28]              32\n          Dropout-12           [-1, 16, 28, 28]               0\n           Conv2d-13           [-1, 32, 28, 28]           4,608\n             ReLU-14           [-1, 32, 28, 28]               0\n      BatchNorm2d-15           [-1, 32, 28, 28]              64\n        MaxPool2d-16           [-1, 32, 14, 14]               0\n          Dropout-17           [-1, 32, 14, 14]               0\n           Conv2d-18           [-1, 24, 14, 14]             768\n             ReLU-19           [-1, 24, 14, 14]               0\n      BatchNorm2d-20           [-1, 24, 14, 14]              48\n          Dropout-21           [-1, 24, 14, 14]               0\n           Conv2d-22           [-1, 20, 14, 14]           4,320\n             ReLU-23           [-1, 20, 14, 14]               0\n      BatchNorm2d-24           [-1, 20, 14, 14]              40\n          Dropout-25           [-1, 20, 14, 14]               0\n           Conv2d-26           [-1, 16, 14, 14]           2,880\n             ReLU-27           [-1, 16, 14, 14]               0\n      BatchNorm2d-28           [-1, 16, 14, 14]              32\n        MaxPool2d-29             [-1, 16, 7, 7]               0\n          Dropout-30             [-1, 16, 7, 7]               0\n           Conv2d-31             [-1, 16, 5, 5]           2,320\n             ReLU-32             [-1, 16, 5, 5]               0\n      BatchNorm2d-33             [-1, 16, 5, 5]              32\n           Conv2d-34             [-1, 10, 3, 3]           1,450\n        AvgPool2d-35             [-1, 10, 1, 1]               0\n================================================================\nTotal params: 19,298\nTrainable params: 19,298\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 1.89\nParams size (MB): 0.07\nEstimated Total Size (MB): 1.96\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "model = MNISTNet().to(device)\n",
    "summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "init_lr = 0.03\n",
    "optimizer = optim.Adam(model.parameters(), lr = init_lr)\n",
    "def adjust_lr(optimizer, epoch):\n",
    "    for param_group in optimizer.param_groups:\n",
    "      init_lr = param_group['lr']\n",
    "    lr = max(round(init_lr * 1/(1 + np.pi/50 * epoch), 10), 0.0005)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPOCH 1 / 20\n",
      "\n",
      "Train set: Average loss: 0.0026432930026203394, Train Accuracy: 53547/60000 (89.245), learning rate : 0.03\n",
      "\n",
      "Test set: Average loss: 0.10942014145851135, Test Accuracy: 9649/10000 (96.49)\n",
      "EPOCH 2 / 20\n",
      "\n",
      "Train set: Average loss: 0.0008098260150291026, Train Accuracy: 58071/60000 (96.785), learning rate : 0.0282264781\n",
      "\n",
      "Test set: Average loss: 0.039476048254966735, Test Accuracy: 9871/10000 (98.71)\n",
      "EPOCH 3 / 20\n",
      "\n",
      "Train set: Average loss: 0.0006614654557779431, Train Accuracy: 58420/60000 (97.36666666666666), learning rate : 0.0250754092\n",
      "\n",
      "Test set: Average loss: 0.056418291664123536, Test Accuracy: 9801/10000 (98.01)\n",
      "EPOCH 4 / 20\n",
      "\n",
      "Train set: Average loss: 0.0005489302566275001, Train Accuracy: 58669/60000 (97.78166666666667), learning rate : 0.0210984459\n",
      "\n",
      "Test set: Average loss: 0.036996469521522524, Test Accuracy: 9883/10000 (98.83)\n",
      "EPOCH 5 / 20\n",
      "\n",
      "Train set: Average loss: 0.0004561867972370237, Train Accuracy: 58945/60000 (98.24166666666666), learning rate : 0.0168608517\n",
      "\n",
      "Test set: Average loss: 0.03509839081764221, Test Accuracy: 9896/10000 (98.96)\n",
      "EPOCH 6 / 20\n",
      "\n",
      "Train set: Average loss: 0.0004000714106950909, Train Accuracy: 59057/60000 (98.42833333333333), learning rate : 0.0128301433\n",
      "\n",
      "Test set: Average loss: 0.026706042718887327, Test Accuracy: 9918/10000 (99.18)\n",
      "EPOCH 7 / 20\n",
      "\n",
      "Train set: Average loss: 0.0003539358149282634, Train Accuracy: 59157/60000 (98.595), learning rate : 0.0093175207\n",
      "\n",
      "Test set: Average loss: 0.021590550541877748, Test Accuracy: 9939/10000 (99.39)\n",
      "EPOCH 8 / 20\n",
      "\n",
      "Train set: Average loss: 0.00029999634716659784, Train Accuracy: 59256/60000 (98.76), learning rate : 0.006471296\n",
      "\n",
      "Test set: Average loss: 0.02431331422328949, Test Accuracy: 9926/10000 (99.26)\n",
      "EPOCH 9 / 20\n",
      "\n",
      "Train set: Average loss: 0.0002923529245890677, Train Accuracy: 59289/60000 (98.815), learning rate : 0.0043065752\n",
      "\n",
      "Test set: Average loss: 0.019311446452140807, Test Accuracy: 9929/10000 (99.29)\n",
      "EPOCH 10 / 20\n",
      "\n",
      "Train set: Average loss: 0.0002751487772911787, Train Accuracy: 59339/60000 (98.89833333333333), learning rate : 0.0027509498\n",
      "\n",
      "Test set: Average loss: 0.019895879530906678, Test Accuracy: 9937/10000 (99.37)\n",
      "EPOCH 11 / 20\n",
      "\n",
      "Train set: Average loss: 0.00025620951782912016, Train Accuracy: 59396/60000 (98.99333333333334), learning rate : 0.0016894421\n",
      "\n",
      "Test set: Average loss: 0.019438749074935913, Test Accuracy: 9939/10000 (99.39)\n",
      "EPOCH 12 / 20\n",
      "\n",
      "Train set: Average loss: 0.00024262574152089655, Train Accuracy: 59408/60000 (99.01333333333334), learning rate : 0.0009989899\n",
      "\n",
      "Test set: Average loss: 0.017729833579063416, Test Accuracy: 9944/10000 (99.44)\n",
      "EPOCH 13 / 20\n",
      "\n",
      "Train set: Average loss: 0.00022912322310730815, Train Accuracy: 59452/60000 (99.08666666666667), learning rate : 0.0005695553\n",
      "\n",
      "Test set: Average loss: 0.0178493442773819, Test Accuracy: 9942/10000 (99.42)\n",
      "EPOCH 14 / 20\n",
      "\n",
      "Train set: Average loss: 0.00021683053637389094, Train Accuracy: 59471/60000 (99.11833333333334), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.017526978182792665, Test Accuracy: 9945/10000 (99.45)\n",
      "EPOCH 15 / 20\n",
      "\n",
      "Train set: Average loss: 0.00022561382502317429, Train Accuracy: 59435/60000 (99.05833333333334), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.017690569496154784, Test Accuracy: 9942/10000 (99.42)\n",
      "EPOCH 16 / 20\n",
      "\n",
      "Train set: Average loss: 0.00021665678650606424, Train Accuracy: 59476/60000 (99.12666666666667), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.01729964790344238, Test Accuracy: 9942/10000 (99.42)\n",
      "EPOCH 17 / 20\n",
      "\n",
      "Train set: Average loss: 0.00021903753804508597, Train Accuracy: 59478/60000 (99.13), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.01717167663574219, Test Accuracy: 9944/10000 (99.44)\n",
      "EPOCH 18 / 20\n",
      "\n",
      "Train set: Average loss: 0.00020995161321479827, Train Accuracy: 59483/60000 (99.13833333333334), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.01720467960834503, Test Accuracy: 9942/10000 (99.42)\n",
      "EPOCH 19 / 20\n",
      "\n",
      "Train set: Average loss: 0.0002187386417062953, Train Accuracy: 59462/60000 (99.10333333333334), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.01703386287689209, Test Accuracy: 9946/10000 (99.46)\n",
      "EPOCH 20 / 20\n",
      "\n",
      "Train set: Average loss: 0.0002062982675852254, Train Accuracy: 59469/60000 (99.115), learning rate : 0.0005\n",
      "\n",
      "Test set: Average loss: 0.016964736247062684, Test Accuracy: 9947/10000 (99.47)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    print(f'EPOCH {epoch + 1} / {20}')\n",
    "    adjust_lr(optimizer,epoch)\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}